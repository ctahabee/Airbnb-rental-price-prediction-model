#### Initial exploration

For my initial exploration, my goal was to identify key features that could be relevant for my analysis and/or would require additional cleaning/processing for my subsequent modelling work. After a manual run down of each variable using the names(), str() and summary() functions - I identified a number of variables to narrow down for my analysis and include in the full mod for my initial feature selection. There were a number of variables that repeated the same information (eg. neighbourhood, smart location, city, state etc.) for which I only included the variable with the most granulated information (i.e. neighbourhood cleansed) for the stepwise selection, and ultimately in my model building. Additionally, variables with only one factor level were dropped for the full mod parameter of my feature selection. 
Data Cleaning

I noticed that several logical vectors (instant bookable, is business travel ready etc.), and factors (e.g. neighborhood cleansed, property type etc.) were recorded as character vectors, for which I implemented the relevant adjustments to convert them into the appropriate data type. A lot of the variables also had missing values, for which I imputed the mean values of the column for all the numeric vectors. Some factors had levels that were not included in the test set, for which I implemented factor collapse from the forcats package to make reasonable adjustments (for example, for the property type variable, rows that fell into the ‘Castle’ category were collapsed into the ‘Other’ category). I was also interested in doing some text processing with the amenities column since it was originally included in the dataset as unstructured text data – and I figured it would be reasonable to include amenities as a feature in my model. Accordingly, I constructed an additional ‘amenities count’ column by counting the number of words within each row of the amenities column separated by commas. 
Models and Feature Selection

There were 4 different subsets of features that I ultimately did my modeling work on – the first subset included a narrow group of features that I thought would reasonably explain rental price (as we discussed in class), the second subset included all the features identified via the hybrid stepwise selection method, and the third subset included all the features identified in the second group + the amenities count feature that I constructed by parsing the amenities column. I also ended up adding some extra features that I thought would be sensible to include in running model 7 (linear regression) as well as in my gradient boosting model. For the gradient boosting model, I added and got rid of features based on trial and error on the resulting RMSEs. For each group of features, I ran i) a linear regression model, ii) a decision tree using rpart, iii) a random forest model, and iv) a gradient boosting model. Notably, in the interest of time, I only ran the random forest model on the features identified via hybrid stepwise selection as well as amenities count.  

#### Model Comparison

Ultimately, the gradient boosting model with ntrees = 1000, intreaction depth = 11, and shrinkage = 0.02 on the features identified via the hybrid stepwise selection method as well as the amenities count feature outputted the best results (i.e. lowest RMSE). Notably, I tried several different values for the parameters within the gbm function, and the aforementioned parameters gave me the best results. My final RMSE on the Kaggle private leaderboard ended up being 66.21363.


![alt text](https://i.ibb.co/2nx3gn1/Screen-Shot-2021-03-13-at-8-33-01-AM.png)

#### Discussion

The third gradient boosting model performed the best because I ultimately took the time to adjust the features and parameters (ntrees, interaction depth, shrinkage) to optimize the resulting RMSE, i.e. I trained and ran several models using the gradient boosting method, and ultimately chose to include the best one in my report. Notably, I also attempted to run a gradient boosting model with cross validation – however the model failed to run after 48 hours of waiting. Furthermore, while the random forest model showed promising results, it took too long to run initially for me to test out with different subsets of features and parameters. 

#### Future directions

In retrospect, I would definitely spend more time being organized in approaching the work. I would have liked to have a better sense of the data set, and better organize the different groups of features I would ultimately run my model on instead of working with trial and error. Furthermore, I would test out different feature selection methods (manual/correlation, forward selection, backward selection etc.) and additional modeling techniques including xgboost, ranger, gradient boosting with cross validation etc. Notably, I did attempt some of these additional models and ran into errors that I didn’t have the time to get to the bottom of. Thus, my biggest takeaway from this project is to be more structured in approaching the work, and clean the data sufficiently for all the modeling work to avoid running into errors. Furthermore, I also learned to take some time to better understand the nuances of the algorithm behind each modeling technique, so I could better prepare my data initially to run each technique appopriately without running into errors.
